---
title: "User Based Collaboration Filtering with Jester dats set"
output: html_notebook
---
```{r}
library(recommenderlab)
```

```{r}
data("Jester5k")
```

## Jester5K data Set
- 100개의 농담에 대한 5000명의 평가
- -10 ~ 10 점의 rating이 있음

```{r}
dim(Jester5k)
```

```{r}
class(Jester5k@data)
```

```{r}
hist(getRatings(Jester5k), main="Distribution of ratings")
```

## 사용자 기반 협업필터링 구축하기

```{r}
head(as(Jester5k, "matrix")[,1:10])
```

```{r}
set.seed(1)
which_train <- sample(x = c(TRUE, FALSE), size=nrow(Jester5k), replace=TRUE, prob=c(0.8, 0.2))
head(which_train)
```

```{r}
rec_data_train <- Jester5k[which_train, ]
rec_data_test <- Jester5k[!which_train, ]
dim(rec_data_train)
dim(rec_data_test)
```

```{r}
recommender_models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
recommender_models
```

```{r}
recc_model <- Recommender(data = rec_data_train, method = "UBCF")
recc_model
recc_model@model$data
```

```{r}
n_recommended <- 10
recc_predicted <- predict(object=recc_model, newdata=rec_data_test, n=n_recommended)
recc_predicted
```

```{r}
rec_list <- sapply(recc_predicted@items, function(x){
  colnames(Jester5k)[x]
})
```

```{r}
class(rec_list)
```

```{r}
rec_list[1:10]
```

```{r}
number_of_items = sort(unlist(lapply(rec_list, length)), decreasing=TRUE)
table(number_of_items)
```

- 첫 행은 추천된 농담의 갯수이며, 두번째 행은 사용자의 수이다.
- 0개를 추천 받은 사용자가 287명인 이유는, 해당 유저는 모든 농담에 평가를 했기 때문이다.

```{r}
table(rowCounts(Jester5k))
```

- 상기는 평가한 농담의 갯수에 따라 사용자를 카운팅한 결과이다.
- 1,422 명의 사용자가 100개의 농담 모두를 평가했다.
- 80개 이상 평가한 사용자들은 제외하고 다시 구성하도록 한다.

```{r}
model_data = Jester5k[rowCounts(Jester5k) < 80]
dim(model_data)
```

```{r}
boxplot(model_data)
```

```{r}
boxplot(rowMeans(model_data[rowMeans(model_data)>=-5 & rowMeans(model_data)<= 7]))
```

```{r}
model_data = model_data[rowMeans(model_data)>=-5 & rowMeans(model_data)<= 7]
dim(model_data)
```

```{r}
image(model_data,main = "Rating distribution of 100 users")
```

```{r}
items_to_keep <- 30
rating_threshold <- 3
n_fold <- 5 # 5-fold
eval_sets <- evaluationScheme(data=model_data, method = "cross-validation", train=percentage_training, given = items_to_keep, goodRating =rating_threshold, k = n_fold)
```

```{r}
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets
```

```{r}
getData(eval_sets, "train")
```

```{r}
model_to_evaluate <- "UBCF"
model_parameters <- NULL
```

```{r}
eval_recommender <- Recommender(data=getData(eval_sets, "train"),
                                method = model_to_evaluate, parameter = model_parameters)
```

```{r}
items_to_recommend <- 10
eval_prediction <- predict(object = eval_recommender, newdata = getData(eval_sets, "known"),
n=items_to_recommend, type = "ratings")
eval_prediction
```

```{r}
eval_accuracy <- calcPredictionAccuracy(x=eval_prediction, data=getData(eval_sets, "unknown"), byUser = TRUE)
#byUser가 TRUE로 지정되면서 각 사용자들의 대한 정확도가 계산됨
head(eval_accuracy)
```

```{r}
apply(eval_accuracy, 2, mean)
```

```{r}
eval_accuracy <- calcPredictionAccuracy(x=eval_prediction, data=getData(eval_sets, "unknown"), byUser = FALSE)
#byUser를 FALSE로 지정하면 전체 모델의 정확도를 계산가능
head(eval_accuracy)
```

```{r}
results <- evaluate(x=eval_sets, method=model_to_evaluate, n=seq(10, 100, 10))
```

```{r}
head(getConfusionMatrix(results)[[1]])
```

```{r}
columns_to_sum <- c("TP", "FP", "FN", "TN")
indices_summed <- Reduce("+", getConfusionMatrix(results))[columns_to_sum]
head(indices_summed)
```

```{r}
plot(results, annotate = TRUE, main = "ROC curve")
```

- 해당 필터링은 KNN 을 기반으로 구성되었음
- ROC curve에서 그래프의 숫자는 기준이 된 K의 갯수
- K가 30일 때, TPR은 0.7, FPR은 약 0.4로 가장 균형잡힌 성능을 보임을 알 수 있음
- K를 더 높여 40이 되면, TPR은 약 0.75이지만, FPR은 약 0.55 정도로 오류율이 더 높아짐